from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 1ï¸âƒ£ ëª¨ë¸ ì„¤ì •
model_name = "EleutherAI/polyglot-ko-1.3b"
device = "cpu"  # Mac MPS ëŒ€ì‹  CPUë¡œ ê°•ì œ ì„¤ì •
print(f"í˜„ì¬ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# 2ï¸âƒ£ í”„ë¡¬í”„íŠ¸
pitch = 210
speed = 5.8
volume = 68

prompt = f"""
ë„ˆëŠ” í•œêµ­ì–´ ìŠ¤í”¼ì¹˜ ì½”ì¹˜ì•¼.
ë‹¤ìŒ ë°ì´í„°ë¥¼ ë³´ê³  í•œ ë¬¸ì¥ìœ¼ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•´ì¤˜.
- í‰ê·  í”¼ì¹˜: {pitch} Hz
- ì†ë„: {speed} ìŒì ˆ/ì´ˆ
- ë³¼ë¥¨: {volume} dB
"""

# 3ï¸âƒ£ ì‹¤í–‰
inputs = tokenizer(prompt, return_tensors="pt").to(device)
if "token_type_ids" in inputs:
    del inputs["token_type_ids"]

outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    do_sample=True,
    top_p=0.9,
    temperature=0.8
)

# 4ï¸âƒ£ ê²°ê³¼ ì •ì œ
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
result = result.split("dB")[-1].strip()

print("\n[ğŸ” ìŠ¤í”¼ì¹˜ í”¼ë“œë°± ê²°ê³¼]\n")
print(result)
